# This workflow runs performance tests for the Pet Store API using Gatling
# It can run all tests or specific test suites (pet, store, user) and publishes results to GitHub Pages
name: Performance Tests

# Define required permissions for GitHub Actions
permissions:
  contents: write # Needed for checking out code
  pages: write # Required for publishing test reports
  id-token: write # Required for deployment authentication

# Configure when the workflow should run
on:
  push:
    branches: ["main"]
    paths:
      - "api-test-performance/**"
      - ".github/workflows/performance-test.yml"
  workflow_dispatch:
    inputs:
      test_type:
        description: "Test suite to run"
        required: true
        default: "all"
        type: choice
        options:
          - all
          - pet
          - store
          - user
      debug:
        description: "Run in debug mode"
        required: false
        type: boolean
        default: false

jobs:
  test:
    timeout-minutes: 60
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    env:
      REPORT_PATH: ""

    steps:
      # Step 1: Check out the test repository containing our performance tests
      - name: Checkout Test Repository
        uses: actions/checkout@v4

      # Step 2: Check out the Swagger Petstore application that we'll be testing
      - name: Checkout Swagger Petstore
        uses: actions/checkout@v4
        with:
          repository: swagger-api/swagger-petstore
          path: swagger-petstore

      # Step 3: Set up Java environment for running the Petstore application
      - name: Set up JDK
        uses: actions/setup-java@v3
        with:
          java-version: "8"
          distribution: "temurin"
          cache: "maven"

      # Step 4: Build and start the Petstore application in a Docker container
      - name: Build & Start Petstore
        run: |
          cd swagger-petstore
          mvn clean package -DskipTests
          docker build -t swaggerapi/petstore3:unstable .
          docker run --name swaggerapi-petstore3 -d -p 8080:8080 swaggerapi/petstore3:unstable
          # Wait for the application to be ready (timeout after 60 seconds)
          timeout 60 bash -c 'while [[ "$(curl -s -o /dev/null -w ''%{http_code}'' http://localhost:8080/api/v3/openapi.json)" != "200" ]]; do sleep 5; done' || false

      # Step 5: Set up Node.js environment for running Gatling tests
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: lts/*

      # Step 6: Install project dependencies
      - name: Install Dependencies
        working-directory: ./api-test-performance
        run: npm ci

      # Step 7: Configure test parameters based on package.conf settings
      - name: Set Test Configuration
        working-directory: ./api-test-performance
        run: |
          cat > .env << EOF
          # Global simulation settings from package.conf
          RAMP_UP_DURATION=60
          RAMP_DOWN_DURATION=30

          # Pet simulation variables
          PET_RAMP_DURATION=60
          CREATE_PETS=100
          READ_PETS=200

          # Store simulation variables
          ORDER_RAMP_DURATION=60
          CREATE_ORDERS=150
          READ_ORDERS=300
          DELETE_ORDERS=50
          READ_INVENTORY=200

          # User simulation variables
          USER_RAMP_DURATION=60
          CREATE_USERS=80
          READ_USERS=160
          UPDATE_USERS=64
          DELETE_USERS=40
          AUTH_USERS=200
          EOF

          # Export all non-comment lines to GitHub Actions environment
          while IFS= read -r line || [[ -n "$line" ]]; do
            if [[ ! "$line" =~ ^#.*$ ]] && [[ ! -z "$line" ]]; then
              echo "$line" >> $GITHUB_ENV
            fi
          done < .env

      # Step 8: Execute the performance tests based on the selected test type
      - name: Run Performance Tests
        working-directory: ./api-test-performance
        run: |
          source .env
          if [[ "${{ github.event.inputs.test_type }}" == "pet" ]]; then
            npm run pet-simulation
          elif [[ "${{ github.event.inputs.test_type }}" == "store" ]]; then
            npm run store-simulation
          elif [[ "${{ github.event.inputs.test_type }}" == "user" ]]; then
            npm run user-simulation
          else
            npm run all-simulations
          fi

      # Step 9: Prepare the test reports for publication

      - name: Prepare Report Directory
        id: prepare_report
        run: |
          # Create the base directory structure for GitHub Pages
          mkdir -p gh-pages/api-test-performance

          # Find and sort simulation directories in reverse order (newest first)
          SIMS=$(find "api-test-performance/target/gatling" -maxdepth 1 -type d -name "jssimulation-*" | sort -r)

          if [ -z "$SIMS" ]; then
            echo "Error: No simulation directories found"
            exit 1
          fi

          # Create an array to store simulation names
          declare -a sim_names

          # Create an index.html file with styling and structure
          cat > gh-pages/api-test-performance/index.html << EOF
          <!DOCTYPE html>
          <html>
          <head>
            <title>Performance Test Reports</title>
            <style>
              body { font-family: Arial, sans-serif; margin: 40px; }
              h1 { color: #333; }
              ul { list-style-type: none; padding: 0; }
              li { margin: 10px 0; }
              a { color: #0366d6; text-decoration: none; }
              a:hover { text-decoration: underline; }
              .timestamp { 
                color: #666;
                font-size: 0.9em;
                margin-left: 10px;
              }
            </style>
          </head>
          <body>
            <h1>Performance Test Reports</h1>
            <ul>
          EOF

          # Function to determine simulation type from the simulation file content
          get_simulation_type() {
            local sim_dir="$1"
            local simulation_log="$sim_dir/simulation.log"
            
            # First, try to extract the simulation name from the log file
            local simulation_name=""
            if [ -f "$simulation_log" ]; then
              # Look for the simulation class name in the log
              simulation_name=$(grep -o "pet\|store\|user" "$simulation_log" | head -n 1)
            fi
            
            # Based on the simulation name, return the appropriate display name
            case "$simulation_name" in
              "pet")
                echo "Pet Management Test Suite"
                ;;
              "store")
                echo "Store Operations Test Suite"
                ;;
              "user")
                echo "User Management Test Suite"
                ;;
              *)
                # If we couldn't determine the type, check the original typescript files
                if [ -f "api-test-performance/src/simulations/pet.gatling.ts" ] && grep -q "pet.gatling.ts" "$simulation_log"; then
                  echo "Pet Management Test Suite"
                elif [ -f "api-test-performance/src/simulations/store.gatling.ts" ] && grep -q "store.gatling.ts" "$simulation_log"; then
                  echo "Store Operations Test Suite"
                elif [ -f "api-test-performance/src/simulations/user.gatling.ts" ] && grep -q "user.gatling.ts" "$simulation_log"; then
                  echo "User Management Test Suite"
                else
                  echo "Unknown Test Suite"
                fi
                ;;
            esac
          }

          # Process each simulation directory and add it to the index
          for sim in $SIMS; do
            SIM_NAME=$(basename "$sim")
            # Copy the simulation directory to the pages directory
            cp -r "$sim" "gh-pages/api-test-performance/"
            sim_names+=("$SIM_NAME")
            
            # Extract and format timestamp for display
            TIMESTAMP=$(echo $SIM_NAME | grep -o "[0-9]\{14\}")
            FORMATTED_TIME=$(date -d "${TIMESTAMP:0:8} ${TIMESTAMP:8:2}:${TIMESTAMP:10:2}:${TIMESTAMP:12:2}" "+%Y-%m-%d %H:%M:%S")

            # Get the simulation type using our helper function
            DISPLAY_NAME=$(get_simulation_type "$sim")

            # Add entry to index.html with clean name and timestamp
            echo "<li><a href='./${SIM_NAME}/index.html'>${DISPLAY_NAME}</a><span class='timestamp'>${FORMATTED_TIME}</span></li>" >> gh-pages/api-test-performance/index.html
            
            # Debug output
            echo "Processed simulation: $DISPLAY_NAME at $FORMATTED_TIME"
          done

          # Close the HTML document
          echo "</ul></body></html>" >> gh-pages/api-test-performance/index.html

          # Save the first simulation name for the report URL
          echo "simulation_name=${sim_names[0]}" >> "$GITHUB_OUTPUT"
          echo "All simulations processed successfully"
      # Step 10: Configure GitHub Pages for report publishing
      - name: Setup Pages
        uses: actions/configure-pages@v4

      # Step 11: Upload the generated reports as a GitHub Pages artifact
      - name: Upload Pages Artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: gh-pages

      # Step 12: Deploy the reports to GitHub Pages
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      # Step 13: Output the URLs where reports can be accessed
      - name: Print Report URLs
        run: |
          echo "Performance Test Reports will be available at:"
          echo "Main index: https://alexandervegazo26.github.io/pet-store-api-test/api-test-performance/"
          echo "Latest report: https://alexandervegazo26.github.io/pet-store-api-test/api-test-performance/${{ steps.prepare_report.outputs.simulation_name }}"

      # Step 14: Wait for GitHub Pages deployment to complete
      - name: Wait for GitHub Pages
        run: sleep 30

      # Step 15: Clean up Docker resources
      - name: Cleanup
        if: always()
        run: |
          docker stop swaggerapi-petstore3 || true
          docker rm swaggerapi-petstore3 || true
          docker image rm swaggerapi/petstore3:unstable || true
          docker system prune -f

# Configure concurrent workflow runs
concurrency:
  group: "petstore-${{ github.ref }}"
  cancel-in-progress: false
